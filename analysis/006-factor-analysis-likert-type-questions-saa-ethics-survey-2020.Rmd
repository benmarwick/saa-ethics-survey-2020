---
title: "Factor Analysis of Likert-type Questions in the Survey for the Revision of the SAA's Principles of Archaeological Ethics"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  bookdown::html_document2:
  number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      fig.retina = 2,
                      fig.width = 10)
# working from https://pubs.acs.org.sci-hub.se/doi/pdfplus/10.1021/bk-2017-1260.ch007
```

```{r}
library(psych)
library(tidyverse)
library(likert)
library(knitr)
library(patchwork)

# load our custom functions
source(here::here("analysis/functions.R"))
load_survey_data()
```

A factor analysis is a model of the measurement of one or more latent variables. This latent variable cannot be directly measured with a single variable (e.g. intelligence, social anxiety, soil health). Instead, it is seen through the relationships it causes in a set of outcome variables.

We focus on the questions about the nine Principles and the Likert-type responses. 

```{r}
  mylevels <- c('Strongly Disagree', 'Disagree', 'Neutral', 'Agree', 'Strongly Agree')
  
  output_with_ID <- 
  survey_data %>% 
  select(!!likert_q,  
         # variables we may want to group by 
         `Response ID`,
         `Are you a current SAA member?`) %>% 
  drop_na() %>% 
  mutate_at(vars(-`Response ID`,
                 -`Are you a current SAA member?`), 
            ~factor(., levels=mylevels)) 
  
  output <- 
  output_with_ID %>% 
  select(-`Response ID`,
         -`Are you a current SAA member?`) %>% 
  as.data.frame()
  
  long_names <- names(output)
  short_names  <- str_extract(names(output), ".*\\d")
  short_principles <- 
    c("Stewardship",
      "Accountability",
      "Commercialization",
      "Public Education",
      "Intellectual Property",
      "Public Reporting",
      "Records & Preservation",
      "Training & Resources",
      "Safe Edu & Workplace"
      )
  
names(output) <- short_principles
names(output) <- str_remove_all(short_principles, " ")
```

### Testing the assumptions for Factor Analysis

We assume that there are at least some correlations among the variables so that coherent factors can be identified. There should be some degree of collinearity among the variables but not an extreme degree or singularity among the variables.

We explore our data to see if they are suitable for factor analysis. These tests check to see whether our matrix is significantly different from an identity matrix.

```{r}
# test the dataset for factor analysis suitability. Two existing methods are the Bartlett’s Test of Sphericity and the Kaiser, Meyer, Olkin (KMO) Measure of Sampling Adequacy (MSA).

# The former tests whether a matrix is significantly different from an identity matrix.

# https://easystats.github.io/parameters/articles/efa_cfa.html
library(tidyverse)
library(parameters)
library(psych)

# Check factor structure
check_fs <- check_factorstructure(data.matrix(output))

msg <-  str_remove_all(attr(check_fs, "text"), "-")

check_fs
```

We explore the structure in the data using a hierarchical cluster analysis where distances are calculated using polychoric correlation. This is a correlation method for ordinal responses such as likert-type data where we don't know the exact distances between each measurement unit. For example, is the difference between "Strongly Agree" and "Agree" the same is the difference between "Disagree" and "Strongly Disagree"? Are these distances the same for each person? We don't know. 

### Correlations

This plot below shows all pairwise correlations between the questions about the nine Principles. We use a hierarchical cluster analysis to identify similar correlations and these are indicated by the black squares. We can see responses to these questions are mostly positively correlated with each other. The 'Stewardship' principle less so than the others. 

```{r corr}

library(psych)
r.poly <- polychoric(data.matrix(output)) 

library(corrplot)
# https://rpubs.com/melike/corrplot

corrplot_output <- ~{
 corrplot(
   r.poly$rho,
   method = "square",
   outline = FALSE,
   addgrid.col = "darkgray",
   rect.lwd = 4,
   order = "hclust",
   rect.col = "black",
   cl.pos = "b",
   addrect = 3,
   tl.col = "indianred4",
   addCoef.col = "white", 
   number.digits = 2, 
   number.cex = 0.75,
   # diag = FALSE,
   #type="lower"
 )
}

cowplot::plot_grid(corrplot_output)
ggsave(here::here(paste0('figures/nine-principles-factor-corrplot.png')),
       height = 8,
       width = 8)

 
```

### Hierarchical Cluster Analysis

We computed a hierarchical cluster analysis on the correlations to see which questions tend to be similar in how people responded. For example, we can see that responses to questions about the principles on public education and accountability are closely correlated. 

```{r iclust-eda}
# hierarchical cluster analysis 
# https://cran.r-project.org/web/packages/psychTools/vignettes/factor.pdf
library(psych)

ic.poly <- 
  ~{
    iclust.diagram.custom( 
     iclust(r.poly$rho,
         title = "", 
         plot = FALSE),
     cex = 1,
     cex.in.circle = 0.65,
     cex.variables = 1,
     cex.arrow.digits = 0.65,
     main = "")
  }

cowplot::plot_grid(ic.poly)

ggsave(here::here(paste0('figures/nine-principles-factor-iclust.png')),
       height = 5,
       width = 7)

```

### Optimum number of latent factors

Next we can explore to find the optimum number of factors to group the correlated questions into. To do this need to identify how many latent factors are present in our data. 

```{r}
# How many factors to retain in Factor Analysis 
n <- n_factors(data.matrix(output))

# https://www.uwo.ca/fhs/tc/labs/10.FactorAnalysis.pdf
# fa.parallel(data.matrix(output))
# https://cran.r-project.org/web/packages/psychTools/vignettes/factor.pdf
# nfactors(data.matrix(output))

library(see)
library(ggtext)
how_many <- plot(n) + 
   geom_textbox(data = tibble(x = 5, 
                              y = 0.3, 
                              label = (msg), 
                              fill = NA),
                aes(label = label),
                width = unit(0.55, "npc")) +
  labs(title = "" ) +
  theme_modern() # maybe 2?

how_many
```

After exploring, we can confirm the optimum number of latent factors using Confirmatory Factor Analysis (CFA). This allows us to evaluate several models of our data using different numbers of latent factors. This will help us make a good choice the best number of factors to extract. 

```{r}
# How can we statistically test if that’s actually the case? This can be done using Confirmatory Factor Analysis (CFA), that bridges factor analysis with Structural Equation Modelling (SEM).

library(lavaan)
library(performance)

names(output) <- str_remove_all(short_principles, " |&")

# https://easystats.github.io/parameters/articles/efa_cfa.html
# Partition the data
partitions <- 
  data_partition(data.matrix(output), 
                 training_proportion = 0.8)

training <- partitions$training
test <- partitions$test

structure_1 <- psych::fa(training, nfactors = 1) %>% 
  efa_to_cfa()
structure_2 <- psych::fa(training, nfactors = 2)  %>% 
  efa_to_cfa()
structure_3 <- psych::fa(training, nfactors = 3)  %>% 
  efa_to_cfa()
structure_4 <- psych::fa(training, nfactors = 4)  %>% 
  efa_to_cfa()
structure_5 <- psych::fa(training, nfactors = 5)  %>% 
  efa_to_cfa()

# Investigate how a model looks
# structure_1
# structure_2
# structure_3

# Fit and Compare models
f1 <- lavaan::cfa(structure_1, data = test)
f2 <- lavaan::cfa(structure_2, data = test)
f3 <- lavaan::cfa(structure_3, data = test)
f4 <- lavaan::cfa(structure_4, data = test)
f5 <- lavaan::cfa(structure_5, data = test)

cfa_out <- 
 compare_performance(f1, f2, f3, f4, f5, rank = TRUE)
# 2 seem best, store top ranked number of factors for later
n_factors <- parse_number(cfa_out$Model[1])
n_factors
```

The results confirm that `r n_factors` factors are best for our data. Now we compute a latent variable exploratory factor analysis (EFA) using `r n_factors` factors. We can visualise the output in a scatterplot, showing how responses to questions about the Principles cluster together. 

```{r}
# latent variable exploratory factor analysis (EFA)
# Fit an EFA with 2 factors
efa <- psych::fa(data.matrix(output), 
                 # https://rpubs.com/ranvirkumarsah/Intro2PCA_EFA
                 rotate = "oblimin",
                 nfactors = n_factors,
                 cor="poly") 

# MR1 and MR2 scores per respondent in dim(efa$scores)
# could be useful for modelling with other variables
# same as dim(output)

# fa.diagram(efa)
# cor.plot(efa, numbers=TRUE)

efa_model_params <- efa %>% 
  model_parameters(
                  threshold = "max", 
                   digits = 3
                   )

# efa_model_params
```

### Exploratory Factor Analysis

```{r}

# watch out for different sorting in efa and efa_model_params
efa_tbl <- 
tibble(mr1 = efa$loadings[,1],
       mr2 = efa$loadings[,2],
       var = efa_model_params$Variable,
       # the percentage of variance that can be
       # explained by the retained factors
       # https://www.geo.fu-berlin.de/en/v/soga/Geodata-analysis/factor-analysis/A-simple-example-of-FA/index.html
       #  the fraction of the variable’s total 
       # variance explained by the factor 
       communalities = efa$communalities,
       fac = ifelse(!is.na(efa_model_params$MR1), 1, 2)) %>% 
  mutate(var1 = short_principles)
  
## Generate a plot showing how the items load on each factor

library(ggrepel)

efa_plot <- 
ggplot(efa_tbl) +
  aes(mr1,
      mr2,
      label = var1,
      size = communalities,
      colour = as.factor(fac)) +
  geom_point() +
  geom_text_repel(size = 6, 
                 # nudge_x = 0.129,
                 # nudge_y = 0.129,
                 min.segment.length = 1,
                 segment.colour = "grey80",
                 force = 180,
                 bg.r = 0.1,
                 bg.color = "white"
                 ) +
  theme_minimal(base_size = 18)  +
  labs(x = "Minimum residual 1",
       y = "Minimum residual 2") +
  theme(legend.position = c(0.8, 0.9)) +
  guides(colour = FALSE) +
  coord_equal()

# efa_plot
```

The plot on the left shows a path diagram representation. The square boxes are the observed variables, and the ovals are the unobserved, latent factors. The straight arrows are the loadings, the correlation between the factor and the observed variable(s). This helps us to identify which item belong in which factor, and the strength of its contribution. 

Factor loading can be classified based on their magnitude.
* 0.50 or more - Practically significant
* 0.40 to 0.49 - More important
* 0.30 to 0.39 - Minimum consideration level

```{r}
library(patchwork)

# corrplot_output
# corrplot_output

wrap_plots( wrap_elements( ~fa.diagram(efa)), 
            efa_plot) + 
    plot_annotation(tag_levels = 'A') 

ggsave(here::here(paste0('figures/nine-principles-factor-panel.png')),
       height = 6,
       width = 13)
  
```

```{r}
# for tSAAR
library(cowplot)

plot_grid(ic.poly, 
          efa_plot,
          labels="auto") 
    

ggsave(here::here(paste0('figures/nine-principles-factor-panel-tsaar.jpg')),
       height = 6,
       width = 13,
       dpi = 600)
  
```



